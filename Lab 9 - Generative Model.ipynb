{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Generative Models\n",
    "\n",
    "## A: [*Bayesian Linear Regression* ](#partA); B: [*Variational Autoencoder*](#partB)\n",
    "\n",
    "[**Haiping Lu**](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) -  [COM4509/6509 MLAI2020](https://github.com/maalvarezl/MLAI) @ The University of Sheffield\n",
    "\n",
    "**Sources**: Part A is based on the [Bayesian regression with linear basis function models](https://github.com/krasserm/bayesian-machine-learning/blob/dev/bayesian-linear-regression/bayesian_linear_regression.ipynb)  notebook by [Martin Krasser](https://github.com/krasserm). Part B is based on Lab 5 of my [SimplyDeep](https://github.com/haipinglu/SimplyDeep/) notebooks.\n",
    "\n",
    "There are *three* questions in this notebook.\n",
    "\n",
    "**Suggested reading**: \n",
    "* [Bayesian linear regression on Wiki](https://en.wikipedia.org/wiki/Bayesian_linear_regression)\n",
    "* Bayesian linear regression (Section 3.3) in the [PRML book](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "* [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "* [Variational autoencoders notes](https://deepgenerativemodels.github.io/notes/vae/)\n",
    "\n",
    "## Why\n",
    "\n",
    "[**Generative models**](https://en.wikipedia.org/wiki/Generative_model) can estimate the probability of the instance, and also the probability of a class label. One of their important applications is [uncertainty quantification](https://en.wikipedia.org/wiki/Uncertainty_quantification). \n",
    "\n",
    "We will start from the generative version of linear regression, our most basic machine learning model. The classic generative formulation of linear regression is called **Bayesian linear regression** or simply **Bayesian regression**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partA'></a>A: Bayesian Linear Regression\n",
    "\n",
    "\n",
    "## A1. Recap of linear basis function models\n",
    "\n",
    "*Note that the notations could be different from previous notations.*\n",
    "\n",
    "Linear regression models share the property of being linear in their parameters but not necessarily in their input variables. Using non-linear basis functions of input variables, linear models are able model arbitrary non-linearities from input variables to targets. Polynomial regression is such an example and will be demonstrated later. A linear regression model $y(\\mathbf{x}, \\mathbf{w})$ can therefore be defined more generally as\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1}{w_j \\phi_j(\\mathbf{x})} = \\sum_{j=0}^{M-1}{w_j \\phi_j(\\mathbf{x})} = \\mathbf{w}^T \\boldsymbol\\phi(\\mathbf{x}) \\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\phi_j$ are basis functions and $M$ is the total number of parameters $w_j$ including the bias term $w_0$. Here, we use the convention $\\phi_0(\\mathbf{x}) = 1$. The simplest form of linear regression models are also linear functions of their input variables i.e. the set of basis functions in this case is the identity $\\boldsymbol\\phi(\\mathbf{x}) = \\mathbf{x}$. The target variable $t$ of an observation $\\mathbf{x}$ is given by a deterministic function $y(\\mathbf{x}, \\mathbf{w})$ plus additive random noise $\\epsilon$. \n",
    "\n",
    "$$\n",
    "t = y(\\mathbf{x}, \\mathbf{w}) + \\epsilon \\tag{2}\n",
    "$$\n",
    "\n",
    "We make the assumption that the noise is normally distributed i.e. follows a Gaussian distribution with zero mean and precision (= inverse variance) $\\beta$. The corresponding probabilistic model i.e. the conditional distribution of $t$ given $\\mathbf{x}$ can therefore be written as\n",
    "\n",
    "$$\n",
    "p(t \\lvert \\mathbf{x}, \\mathbf{w}, \\beta) = \n",
    "\\mathcal{N}(t \\lvert y(\\mathbf{x}, \\mathbf{w}), \\beta^{-1}) =\n",
    "\\sqrt{\\beta \\over {2 \\pi}} \\exp\\left(-{\\beta \\over 2} (t - y(\\mathbf{x}, \\mathbf{w}))^2 \\right) \\tag{3}\n",
    "$$\n",
    "\n",
    "where the mean of this distribution is the regression function $y(\\mathbf{x}, \\mathbf{w})$. \n",
    "\n",
    "### Likelihood function\n",
    "\n",
    "For fitting the model and for inference of model parameters we use a training set of $N$ independent and identically distributed (i.i.d.) observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_N$ and their corresponding targets $t_1,\\ldots,t_N$. After combining column vectors $\\mathbf{x}_i$ into matrix $\\mathbf{X}$, where $\\mathbf{X}_{i,:} = \\mathbf{x}_i^T$, and scalar targets $t_i$ into column vector $\\mathbf{t}$ the joint conditional probability of targets $\\mathbf{t}$ given $\\mathbf{X}$ can be formulated as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{t} \\lvert \\mathbf{X}, \\mathbf{w}, \\beta) = \n",
    "\\prod_{i=1}^{N}{\\mathcal{N}(t_i \\lvert \\mathbf{w}^T \\boldsymbol\\phi(\\mathbf{x}_i), \\beta^{-1})} \\tag{4}\n",
    "$$\n",
    "\n",
    "This is a function of parameters $\\mathbf{w}$ and $\\beta$ and is called the *likelihood function*. For better readability, it will be written as $p(\\mathbf{t} \\lvert \\mathbf{w}, \\beta)$ instead of $p(\\mathbf{t} \\lvert \\mathbf{X}, \\mathbf{w}, \\beta)$ from now on. The log of the likelihood function can be written as \n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{t} \\lvert \\mathbf{w}, \\beta) = \n",
    "{N \\over 2} \\log \\beta - \n",
    "{N \\over 2} \\log {2 \\pi} - \n",
    "\\beta E_D(\\mathbf{w}) \\tag{5}\n",
    "$$\n",
    "\n",
    "where $E_D(\\mathbf{w})$ is the sum-of-squares error function coming from the exponent of the likelihood function.\n",
    "\n",
    "$$\n",
    "E_D(\\mathbf{w}) = \n",
    "{1 \\over 2} \\sum_{i=1}^{N}(t_i - \\mathbf{w}^T \\boldsymbol\\phi(\\mathbf{x}_i))^2 = \n",
    "{1 \\over 2} \\lVert \\mathbf{t} - \\boldsymbol\\Phi \\mathbf{w} \\rVert^2 \\tag{6}\n",
    "$$\n",
    "\n",
    "Matrix $\\boldsymbol\\Phi$ is called the *design matrix* and is defined as\n",
    "\n",
    "$$\n",
    "\\boldsymbol\\Phi = \n",
    "\\begin{pmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &  \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\ \n",
    "\\phi_0(\\mathbf{x}_2) &  \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_0(\\mathbf{x}_N) &  \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N)\n",
    "\\end{pmatrix} \\tag{7}\n",
    "$$\n",
    "\n",
    "### Maximum likelihood\n",
    "\n",
    "Maximizing the log likelihood (= minimizing the sum-of-squares error function) w.r.t. $\\mathbf{w}$ gives the maximum likelihood estimate of parameters $\\mathbf{w}$. Maximum likelihood estimation can lead to severe over-fitting if complex models (e.g. polynomial regression models of high order) are fit to datasets of limited size. A common approach to prevent over-fitting is to add a regularization term to the error function. As we will see shortly, this regularization term arises naturally when following a Bayesian approach (more precisely, when defining a prior distribution over parameters $\\mathbf{w}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Bayesian approach to linear regression\n",
    "\n",
    "### Prior and posterior distribution\n",
    "\n",
    "For a Bayesian treatment of linear regression we need a prior probability distribution over model parameters $\\mathbf{w}$. For reasons of simplicity, we will use an isotropic Gaussian distribution over parameters $\\mathbf{w}$ with zero mean:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} \\lvert \\alpha) = \\mathcal{N}(\\mathbf{w} \\lvert \\mathbf{0}, \\alpha^{-1}\\mathbf{I}) \\tag{8}\n",
    "$$\n",
    "\n",
    "An isotropic Gaussian distribution has a diagonal covariance matrix where all diagonal elements have the same variance $\\alpha^{-1}$ ($\\alpha$ is the precision of the prior). A zero mean favors small(er) values of parameters $w_j$ a priori. The prior is [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior) to the likelihood $p(\\mathbf{t} \\lvert \\mathbf{w}, \\beta)$ meaning that the posterior distribution has the same functional form as the prior i.e. is also a Gaussian. In this special case, the posterior has an analytical solution with the following [sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic). \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{m}_N &= \\beta \\mathbf{S}_N \\boldsymbol\\Phi^T \\mathbf{t}  \\tag{9} \\\\\n",
    "\\mathbf{S}_N^{-1} &= \\alpha\\mathbf{I} + \\beta \\boldsymbol\\Phi^T \\boldsymbol\\Phi  \\tag{10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$(9)$ is the mean vector of the posterior and $(10)$ the inverse covariance matrix (= precision matrix). Hence, the posterior distribution can be written as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta) = \\mathcal{N}(\\mathbf{w} \\lvert \\mathbf{m}_N, \\mathbf{S}_N) \\tag{11}\n",
    "$$\n",
    "\n",
    "For the moment, we assume that the values of $\\alpha$ and $\\beta$ are known. Since the posterior is proportional to the product of likelihood and prior, the log of the posterior distribution is proportional to the sum of the log likelihood and the log of the prior\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta) = \n",
    "-\\beta E_D(\\mathbf{w}) - \\alpha E_W(\\mathbf{w}) + \\mathrm{const.} \\tag{12}\n",
    "$$\n",
    "\n",
    "where $E_D(\\mathbf{w})$ is defined by $(6)$ and \n",
    "\n",
    "$$\n",
    "E_W(\\mathbf{w}) = {1 \\over 2} \\mathbf{w}^T \\mathbf{w} \\tag{13}\n",
    "$$\n",
    "\n",
    "Maximizing the log posterior w.r.t. $\\mathbf{w}$ gives the [maximum-a-posteriori](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP) estimate of $\\mathbf{w}$. Maximizing the log posterior is equivalent to minimizing the sum-of-squares error function $E_D$ plus a quadratic regularization term $E_W$. This particular form regularization is known as *L2 regularization* or *weight decay* as it limits the magnitude of weights $w_j$. The contribution of the regularization term is determined by the ratio $\\alpha / \\beta$.\n",
    "\n",
    "### Posterior predictive distribution\n",
    "\n",
    "For making a prediction $t$ at a new location $\\mathbf{x}$ we use the posterior predictive distribution which is defined as\n",
    "\n",
    "$$\n",
    "p(t \\lvert \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) = \n",
    "\\int{p(t \\lvert \\mathbf{x}, \\mathbf{w}, \\beta) p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta) d\\mathbf{w}} \\tag{14}\n",
    "$$\n",
    "\n",
    "The posterior predictive distribution includes uncertainty about parameters $\\mathbf{w}$ into predictions by weighting the conditional distribution $p(t \\lvert \\mathbf{x}, \\mathbf{w}, \\beta)$ with the posterior probability of weights $p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta)$ over the entire weight parameter space. By using the predictive distribution we're not only getting the expected value of $t$ at a new location $\\mathbf{x}$ but also the **uncertainty for that prediction**. In our special case, the posterior predictive distribution is a Gaussian distribution\n",
    "\n",
    "$$\n",
    "p(t \\lvert \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) = \n",
    "\\mathcal{N}(t \\lvert \\mathbf{m}_N^T \\boldsymbol\\phi(\\mathbf{x}), \\sigma_N^2(\\mathbf{x})) \\tag{15}\n",
    "$$\n",
    "\n",
    "where mean $\\mathbf{m}_N^T \\boldsymbol\\phi(\\mathbf{x})$ is the regression function after $N$ observations and $\\sigma_N^2(\\mathbf{x})$ is the corresponding predictive variance\n",
    "\n",
    "$$\n",
    "\\sigma_N^2(\\mathbf{x}) = {1 \\over \\beta} + \\boldsymbol\\phi(\\mathbf{x})^T \\mathbf{S}_N \\boldsymbol\\phi(\\mathbf{x}) \\tag{16}\n",
    "$$\n",
    "\n",
    "The first term in $(16)$ represents the inherent noise in the data and the second term covers the uncertainty about parameters $\\mathbf{w}$. So far, we have assumed that the values of $\\alpha$ and $\\beta$ are known. In a fully Bayesian treatment, however, we should define priors over $\\alpha$ and $\\beta$ and use the corresponding posteriors to additionally include uncertainties about $\\alpha$ and $\\beta$ into predictions, which will not be covered here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Implementation of Bayesian regression\n",
    "\n",
    "### Posterior and posterior predictive distribution\n",
    "\n",
    "We start with the implementation of the posterior and posterior predictive distributions. Function `posterior` computes the mean and covariance matrix of the posterior distribution and function `posterior_predictive` computes the mean and the variances of the posterior predictive distribution. Here, readability of code and similarity to the mathematical definitions has higher priority than optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def posterior(Phi, t, alpha, beta, return_inverse=False):\n",
    "    \"\"\"Computes mean and covariance matrix of the posterior distribution.\"\"\"\n",
    "    S_N_inv = alpha * np.eye(Phi.shape[1]) + beta * Phi.T.dot(Phi)\n",
    "    S_N = np.linalg.inv(S_N_inv)\n",
    "    m_N = beta * S_N.dot(Phi.T).dot(t)\n",
    "\n",
    "    if return_inverse:\n",
    "        return m_N, S_N, S_N_inv\n",
    "    else:\n",
    "        return m_N, S_N\n",
    "\n",
    "def posterior_predictive(Phi_test, m_N, S_N, beta):\n",
    "    \"\"\"Computes mean and variances of the posterior predictive distribution.\"\"\"\n",
    "    y = Phi_test.dot(m_N)\n",
    "    # Only compute variances (diagonal elements of covariance matrix)\n",
    "    y_var = 1 / beta + np.sum(Phi_test.dot(S_N) * Phi_test, axis=1)\n",
    "    \n",
    "    return y, y_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some plotting functions for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, t):\n",
    "    plt.scatter(x, t, marker='o', c=\"k\", s=20)\n",
    "\n",
    "def plot_truth(x, y, label='Truth'):\n",
    "    plt.plot(x, y, 'k--', label=label)\n",
    "\n",
    "def plot_predictive(x, y, std, y_label='Prediction', std_label='Uncertainty', plot_xy_labels=True):\n",
    "    y = y.ravel()\n",
    "    std = std.ravel()\n",
    "\n",
    "    plt.plot(x, y, label=y_label)\n",
    "    plt.fill_between(x.ravel(), y + std, y - std, alpha = 0.5, label=std_label)\n",
    "\n",
    "    if plot_xy_labels:\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "\n",
    "def plot_posterior_samples(x, ys, plot_xy_labels=True):\n",
    "    plt.plot(x, ys[:, 0], 'r-', alpha=0.5, label='Post. samples')\n",
    "    for i in range(1, ys.shape[1]):\n",
    "        plt.plot(x, ys[:, i], 'r-', alpha=0.5)\n",
    "\n",
    "    if plot_xy_labels:\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "\n",
    "def plot_posterior(mean, cov, w0, w1):\n",
    "    resolution = 100\n",
    "\n",
    "    grid_x = grid_y = np.linspace(-1, 1, resolution)\n",
    "    grid_flat = np.dstack(np.meshgrid(grid_x, grid_y)).reshape(-1, 2)\n",
    "\n",
    "    densities = stats.multivariate_normal.pdf(grid_flat, mean=mean.ravel(), cov=cov).reshape(resolution, resolution)\n",
    "    plt.imshow(densities, origin='lower', extent=(-1, 1, -1, 1))\n",
    "    plt.scatter(w0, w1, marker='x', c=\"r\", s=20, label='Truth')\n",
    "\n",
    "    plt.xlabel('w0')\n",
    "    plt.ylabel('w1')\n",
    "\n",
    "def print_comparison(title, a, b, a_prefix='np', b_prefix='br'):\n",
    "    print(title)\n",
    "    print('-' * len(title))\n",
    "    print(f'{a_prefix}:', a)\n",
    "    print(f'{b_prefix}:', b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example datasets\n",
    "\n",
    "The datasets used in the following examples are based on $N$ scalar observations $x_{i = 1,\\ldots,N}$ which are combined into a $N \\times 1$ matrix $\\mathbf{X}$. Target values $\\mathbf{t}$ are generated from $\\mathbf{X}$ with functions `f` and `g` which also generate random noise whose variance can be specified with the `noise_variance` parameter. We will use `f` for generating noisy samples from a straight line and `g` for generating noisy samples from a sinusoidal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w0 = -0.3\n",
    "f_w1 =  0.5\n",
    "\n",
    "def f(X, noise_variance):\n",
    "    '''Linear function plus noise'''\n",
    "    return f_w0 + f_w1 * X + noise(X.shape, noise_variance)\n",
    "\n",
    "def g(X, noise_variance):\n",
    "    '''Sinusoidial function plus noise'''\n",
    "    return 0.5 + np.sin(2 * np.pi * X) + noise(X.shape, noise_variance)\n",
    "\n",
    "def noise(size, variance):\n",
    "    return np.random.normal(scale=np.sqrt(variance), size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis functions\n",
    "\n",
    "For straight line fitting, a model that is linear in its input variable $x$ is sufficient. Hence, we don't need to transform $x$ with a basis function which is equivalent to using an `identity_basis_function`. For fitting a linear model to a sinusoidal dataset we transform input $x$ with `gaussian_basis_function` and later with `polynomial_basis_function`. These non-linear basis functions are necessary to model the non-linear relationship between input $x$ and target $t$. The design matrix $\\boldsymbol\\Phi$ can be computed from observations $\\mathbf{X}$ and a parametric basis function with function `expand`. This function also prepends a column vector $\\mathbf{1}$ according to $\\phi_0(x) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_basis_function(x):\n",
    "    return x\n",
    "\n",
    "def gaussian_basis_function(x, mu, sigma=0.1):\n",
    "    return np.exp(-0.5 * (x - mu) ** 2 / sigma ** 2)\n",
    "\n",
    "def polynomial_basis_function(x, power):\n",
    "    return x ** power\n",
    "\n",
    "def expand(x, bf, bf_args=None):\n",
    "    if bf_args is None:\n",
    "        return np.concatenate([np.ones(x.shape), bf(x)], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([np.ones(x.shape)] + [bf(x, bf_arg) for bf_arg in bf_args], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight line fitting\n",
    "\n",
    "For straight line fitting, we use a linear regression model of the form $y(x, \\mathbf{w}) = w_0 + w_1 x$ and do Bayesian inference for model parameters $\\mathbf{w}$. Predictions are made with the posterior predictive distribution. Since this model has only two parameters, $w_0$ and $w_1$, we can visualize the posterior density in 2D which is done in the first column of the following output. Rows use an increasing number of training data from a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset sizes\n",
    "N_list = [1, 3, 20]\n",
    "\n",
    "beta = 25.0\n",
    "alpha = 2.0\n",
    "\n",
    "# Training observations in [-1, 1)\n",
    "X = np.random.rand(N_list[-1], 1) * 2 - 1\n",
    "\n",
    "# Training target values\n",
    "t = f(X, noise_variance=1/beta)\n",
    "\n",
    "# Test observations\n",
    "X_test = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Function values without noise \n",
    "y_true = f(X_test, noise_variance=0)\n",
    "    \n",
    "# Design matrix of test observations\n",
    "Phi_test = expand(X_test, identity_basis_function)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    X_N = X[:N]\n",
    "    t_N = t[:N]\n",
    "\n",
    "    # Design matrix of training observations\n",
    "    Phi_N = expand(X_N, identity_basis_function)\n",
    "    \n",
    "    # Mean and covariance matrix of posterior\n",
    "    m_N, S_N = posterior(Phi_N, t_N, alpha, beta)\n",
    "    \n",
    "    # Mean and variances of posterior predictive \n",
    "    y, y_var = posterior_predictive(Phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    # Draw 5 random weight samples from posterior and compute y values\n",
    "    w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = Phi_test.dot(w_samples)\n",
    "    \n",
    "    plt.subplot(len(N_list), 3, i * 3 + 1)\n",
    "    plot_posterior(m_N, S_N, f_w0, f_w1)\n",
    "    plt.title(f'Posterior density (N = {N})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(len(N_list), 3, i * 3 + 2)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true)\n",
    "    plot_posterior_samples(X_test, y_samples)\n",
    "    plt.ylim(-1.5, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(len(N_list), 3, i * 3 + 3)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true, label=None)\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plt.ylim(-1.5, 1.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second column, 5 random weight samples are drawn from the posterior and the corresponding regression lines are plotted in red color. The line resulting from the true parameters, `f_w0` and `f_w1` is plotted as dashed black line and the noisy training data as black dots. The third column shows the mean and the standard deviation of the posterior predictive distribution along with the true model and the training data.\n",
    "\n",
    "It can be clearly seen how the posterior density in the first column gets more sharply peaked as the size of the dataset increases which corresponds to a decrease in the sample variance in the second column and to a decrease in prediction uncertainty as shown in the third column. Also note how prediction uncertainty is higher in regions of less observations. \n",
    "\n",
    "### Gaussian basis functions\n",
    "\n",
    "The following example demonstrates how to fit a Gaussian basis function model to a noisy sinusoidal dataset. It uses 9 Gaussian basis functions with mean values equally distributed over $[0, 1]$ each having a standard deviation of $0.1$. Inference for parameters $\\mathbf{w}$ is done in the same way as in the previous example except that we now infer values for 10 parameters (bias term $w_0$ and $w_1,\\ldots,w_9$ for the 9 basis functions) instead of 2. We therefore cannot display the posterior density unless we selected 2 parameters at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = [3, 8, 20]\n",
    "\n",
    "beta = 25.0\n",
    "alpha = 2.0\n",
    "\n",
    "# Training observations in [-1, 1)\n",
    "X = np.random.rand(N_list[-1], 1)\n",
    "\n",
    "# Training target values\n",
    "t = g(X, noise_variance=1/beta)\n",
    "\n",
    "# Test observations\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Function values without noise \n",
    "y_true = g(X_test, noise_variance=0)\n",
    "    \n",
    "# Design matrix of test observations\n",
    "Phi_test = expand(X_test, bf=gaussian_basis_function, bf_args=np.linspace(0, 1, 9))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    X_N = X[:N]\n",
    "    t_N = t[:N]\n",
    "\n",
    "    # Design matrix of training observations\n",
    "    Phi_N = expand(X_N, bf=gaussian_basis_function, bf_args=np.linspace(0, 1, 9))\n",
    "\n",
    "    # Mean and covariance matrix of posterior\n",
    "    m_N, S_N = posterior(Phi_N, t_N, alpha, beta)\n",
    "    \n",
    "    # Mean and variances of posterior predictive \n",
    "    y, y_var = posterior_predictive(Phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    # Draw 5 random weight samples from posterior and compute y values\n",
    "    w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = Phi_test.dot(w_samples)\n",
    "    \n",
    "    plt.subplot(len(N_list), 2, i * 2 + 1)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true)\n",
    "    plot_posterior_samples(X_test, y_samples)\n",
    "    plt.ylim(-1.0, 2.0)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(len(N_list), 2, i * 2 + 2)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true, label=None)\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plt.ylim(-1.0, 2.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as the size of the dataset increases the posterior sample variance and the prediction uncertainty decreases. Also, regions with less observations have higher prediction uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partB'></a>B: Variational Autoencoder for Deep Generative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional ideas to explore\n",
    "\n",
    "* Explore other VAEs following the [PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE) repository to generate new faces from celebrities.\n",
    "* Study [A Tutorial on Variational Autoencoders with a Concise Keras Implementation](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/) by Louis Tiao and implement the same in pytorch.\n",
    "* Explore [PyroLab 1 - Bayesian Linear Regression for Generative Learning with Pyro](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%201%20-%20Bayesian%20Linear%20Regression%20for%20Generative%20Learning%20with%20Pyro.ipynb)\n",
    "* Explore [PyroLab 2 - Variational Autoencoder for Deep Generative Learning](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%202%20-%20Variational%20Autoencoder%20for%20Deep%20Generative%20Learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
